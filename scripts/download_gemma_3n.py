#!/usr/bin/env python3
"""
Gemma 3N æ¨¡å‹è‡ªåŠ¨ä¸‹è½½å’Œå®‰è£…è„šæœ¬
ä¸“é—¨é’ˆå¯¹ Gemma 3N ç³»åˆ—æ¨¡å‹çš„ä¸‹è½½å’Œè½¬æ¢
"""

import os
import sys
import json
import shutil
from pathlib import Path
import subprocess

def check_dependencies():
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–"""
    print("ğŸ” æ£€æŸ¥ä¾èµ–...")
    
    required_packages = [
        'huggingface_hub',
        'transformers',
        'tensorflow',
        'torch'
    ]
    
    missing_packages = []
    for package in required_packages:
        try:
            __import__(package)
            print(f"  âœ… {package}")
        except ImportError:
            missing_packages.append(package)
            print(f"  âŒ {package}")
    
    if missing_packages:
        print(f"\nğŸ“¦ å®‰è£…ç¼ºå¤±çš„ä¾èµ–åŒ…...")
        try:
            subprocess.check_call([
                sys.executable, '-m', 'pip', 'install'
            ] + missing_packages)
            print("âœ… ä¾èµ–å®‰è£…å®Œæˆ")
        except subprocess.CalledProcessError:
            print("âŒ ä¾èµ–å®‰è£…å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨å®‰è£…:")
            print(f"pip install {' '.join(missing_packages)}")
            return False
    
    return True

def setup_huggingface_auth():
    """è®¾ç½® Hugging Face è®¤è¯"""
    print("\nğŸ” è®¾ç½® Hugging Face è®¤è¯...")
    
    try:
        from huggingface_hub import login, whoami
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»ç™»å½•
        try:
            user_info = whoami()
            print(f"âœ… å·²ç™»å½•ä¸º: {user_info['name']}")
            return True
        except:
            pass
        
        # å°è¯•ä»ç¯å¢ƒå˜é‡è·å– token
        hf_token = os.environ.get('HF_TOKEN')
        if hf_token:
            print("ğŸ”‘ ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„ HF_TOKEN")
            login(token=hf_token)
            return True
        
        # æç¤ºç”¨æˆ·ç™»å½•
        print("è¯·é€‰æ‹©è®¤è¯æ–¹å¼:")
        print("1. ä½¿ç”¨ huggingface-cli login")
        print("2. æ‰‹åŠ¨è¾“å…¥ token")
        print("3. è·³è¿‡è®¤è¯ï¼ˆå¯èƒ½æ— æ³•ä¸‹è½½æŸäº›æ¨¡å‹ï¼‰")
        
        choice = input("è¯·é€‰æ‹© (1-3): ").strip()
        
        if choice == "1":
            print("è¯·åœ¨æ–°ç»ˆç«¯ä¸­è¿è¡Œ: huggingface-cli login")
            input("å®ŒæˆåæŒ‰ Enter ç»§ç»­...")
            return True
        elif choice == "2":
            token = input("è¯·è¾“å…¥æ‚¨çš„ Hugging Face token: ").strip()
            if token:
                login(token=token)
                print("âœ… è®¤è¯æˆåŠŸ")
                return True
        elif choice == "3":
            print("âš ï¸ è·³è¿‡è®¤è¯ï¼ŒæŸäº›æ¨¡å‹å¯èƒ½æ— æ³•ä¸‹è½½")
            return True
        
        return False
        
    except Exception as e:
        print(f"âŒ è®¤è¯è®¾ç½®å¤±è´¥: {e}")
        return False

def list_gemma_3n_models():
    """åˆ—å‡ºå¯ç”¨çš„ Gemma 3N æ¨¡å‹"""
    models = {
        "1": {
            "name": "google/gemma-3n-2b",
            "description": "Gemma 3N 2B - æ¨èç”¨äºç§»åŠ¨è®¾å¤‡",
            "size": "~4GB",
            "ram_requirement": "4GB+"
        },
        "2": {
            "name": "google/gemma-3n-7b", 
            "description": "Gemma 3N 7B - æ›´å¼ºæ€§èƒ½",
            "size": "~13GB",
            "ram_requirement": "8GB+"
        },
        "3": {
            "name": "google/gemma-3n-2b-it",
            "description": "Gemma 3N 2B Instruction Tuned - å¯¹è¯ä¼˜åŒ–",
            "size": "~4GB", 
            "ram_requirement": "4GB+"
        },
        "4": {
            "name": "google/gemma-3n-7b-it",
            "description": "Gemma 3N 7B Instruction Tuned - å¯¹è¯ä¼˜åŒ–",
            "size": "~13GB",
            "ram_requirement": "8GB+"
        }
    }
    
    print("\nğŸ“‹ å¯ç”¨çš„ Gemma 3N æ¨¡å‹:")
    print("=" * 60)
    for key, model in models.items():
        print(f"{key}. {model['name']}")
        print(f"   æè¿°: {model['description']}")
        print(f"   å¤§å°: {model['size']}")
        print(f"   å†…å­˜è¦æ±‚: {model['ram_requirement']}")
        print()
    
    return models

def download_model(model_name, cache_dir="./models_cache"):
    """ä¸‹è½½æŒ‡å®šçš„æ¨¡å‹"""
    print(f"\nğŸ“¥ å¼€å§‹ä¸‹è½½æ¨¡å‹: {model_name}")
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        from huggingface_hub import snapshot_download
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        os.makedirs(cache_dir, exist_ok=True)
        
        print("ğŸ“¦ ä¸‹è½½æ¨¡å‹æ–‡ä»¶...")
        model_path = snapshot_download(
            repo_id=model_name,
            cache_dir=cache_dir,
            resume_download=True
        )
        
        print("ğŸ”¤ ä¸‹è½½åˆ†è¯å™¨...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=cache_dir
        )
        
        print("ğŸ§  åŠ è½½æ¨¡å‹...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            torch_dtype="auto",
            device_map="cpu"  # å¼ºåˆ¶ä½¿ç”¨ CPU ä»¥é¿å… GPU å†…å­˜é—®é¢˜
        )
        
        print("âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ")
        return model, tokenizer, model_path
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")
        return None, None, None

def convert_to_tflite(model, tokenizer, model_name, output_dir="../app/src/main/assets"):
    """è½¬æ¢æ¨¡å‹ä¸º TensorFlow Lite æ ¼å¼"""
    print(f"\nğŸ”„ å¼€å§‹è½¬æ¢æ¨¡å‹ä¸º TensorFlow Lite æ ¼å¼...")
    
    try:
        import tensorflow as tf
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        models_dir = os.path.join(output_dir, "models")
        os.makedirs(models_dir, exist_ok=True)
        
        print("âš ï¸ æ³¨æ„: ç›´æ¥è½¬æ¢å¤§å‹è¯­è¨€æ¨¡å‹ä¸º TFLite æ˜¯ä¸€ä¸ªå¤æ‚çš„è¿‡ç¨‹")
        print("æ¨èä½¿ç”¨ä»¥ä¸‹æ›¿ä»£æ–¹æ¡ˆ:")
        print("1. ä½¿ç”¨é¢„è½¬æ¢çš„ TFLite æ¨¡å‹")
        print("2. ä½¿ç”¨ MediaPipe æˆ–å…¶ä»–ä¼˜åŒ–æ¡†æ¶")
        print("3. ä½¿ç”¨æ¨¡å‹é‡åŒ–å’Œå‰ªææŠ€æœ¯")
        
        # ä¿å­˜åˆ†è¯å™¨è¯æ±‡è¡¨
        print("ğŸ’¾ ä¿å­˜åˆ†è¯å™¨è¯æ±‡è¡¨...")
        vocab_dict = tokenizer.get_vocab()
        vocab_file = os.path.join(output_dir, "vocab.json")
        
        with open(vocab_file, 'w', encoding='utf-8') as f:
            json.dump(vocab_dict, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… è¯æ±‡è¡¨å·²ä¿å­˜: {vocab_file}")
        print(f"   è¯æ±‡è¡¨å¤§å°: {len(vocab_dict)} ä¸ªè¯æ±‡")
        
        # åˆ›å»ºæ¨¡å‹ä¿¡æ¯æ–‡ä»¶
        model_info = {
            "model_name": model_name,
            "model_type": "gemma_3n",
            "vocab_size": len(vocab_dict),
            "max_sequence_length": getattr(tokenizer, 'model_max_length', 2048),
            "status": "downloaded_but_not_converted",
            "note": "Model downloaded successfully. TFLite conversion requires additional optimization."
        }
        
        info_file = os.path.join(models_dir, "model_info.json")
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(model_info, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… æ¨¡å‹ä¿¡æ¯å·²ä¿å­˜: {info_file}")
        
        # åˆ›å»ºå ä½ç¬¦ TFLite æ–‡ä»¶ï¼ˆæ ‡è®°ä¸ºå·²ä¸‹è½½ï¼‰
        placeholder_content = f"GEMMA_3N_MODEL_DOWNLOADED_{model_name.replace('/', '_')}"
        tflite_file = os.path.join(models_dir, "gemma_3n_2b_int8.tflite")
        
        with open(tflite_file, 'w') as f:
            f.write(placeholder_content)
        
        print(f"âœ… å ä½ç¬¦æ–‡ä»¶å·²åˆ›å»º: {tflite_file}")
        
        return vocab_file, info_file
        
    except Exception as e:
        print(f"âŒ è½¬æ¢è¿‡ç¨‹å‡ºé”™: {e}")
        return None, None

def create_optimized_setup():
    """åˆ›å»ºä¼˜åŒ–çš„æ¨¡å‹è®¾ç½®"""
    print("\nğŸ› ï¸ åˆ›å»ºä¼˜åŒ–çš„æ¨¡å‹è®¾ç½®...")
    
    # æ›´æ–° GemmaModelManager ä»¥æ”¯æŒçœŸå®æ¨¡å‹
    model_manager_update = '''
    // åœ¨ GemmaModelManager.kt ä¸­æ·»åŠ ä»¥ä¸‹ä»£ç æ¥æ”¯æŒçœŸå®æ¨¡å‹åŠ è½½
    
    private fun loadRealGemmaModel(): Boolean {
        val modelInfoFile = File(context.getExternalFilesDir("models"), "model_info.json")
        if (modelInfoFile.exists()) {
            try {
                val modelInfo = JSONObject(modelInfoFile.readText())
                val modelType = modelInfo.getString("model_type")
                if (modelType == "gemma_3n") {
                    Log.d(TAG, "Found Gemma 3N model configuration")
                    // è¿™é‡Œå¯ä»¥æ·»åŠ çœŸå®æ¨¡å‹åŠ è½½é€»è¾‘
                    return true
                }
            } catch (e: Exception) {
                Log.e(TAG, "Error reading model info", e)
            }
        }
        return false
    }
    '''
    
    update_file = Path("../app/src/main/java/com/example/gemmaprototype/model/GemmaModelUpdate.kt")
    with open(update_file, 'w') as f:
        f.write(f"""package com.example.gemmaprototype.model

/*
 * Gemma 3N æ¨¡å‹é›†æˆæ›´æ–°
 * 
 * ä½¿ç”¨è¯´æ˜:
 * 1. å°†æ­¤æ–‡ä»¶ä¸­çš„ä»£ç é›†æˆåˆ° GemmaModelManager.kt ä¸­
 * 2. æ ¹æ®ä¸‹è½½çš„æ¨¡å‹è°ƒæ•´åŠ è½½é€»è¾‘
 * 3. å®ç°çœŸå®çš„æ¨¡å‹æ¨ç†åŠŸèƒ½
 */

{model_manager_update}
""")
    
    print(f"âœ… æ¨¡å‹æ›´æ–°ä»£ç å·²åˆ›å»º: {update_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Gemma 3N æ¨¡å‹ä¸‹è½½å’Œå®‰è£…å·¥å…·")
    print("=" * 50)
    
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        return 1
    
    # è®¾ç½®è®¤è¯
    if not setup_huggingface_auth():
        print("âŒ è®¤è¯å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
        return 1
    
    # åˆ—å‡ºå¯ç”¨æ¨¡å‹
    models = list_gemma_3n_models()
    
    # ç”¨æˆ·é€‰æ‹©æ¨¡å‹
    while True:
        choice = input("è¯·é€‰æ‹©è¦ä¸‹è½½çš„æ¨¡å‹ (1-4): ").strip()
        if choice in models:
            selected_model = models[choice]
            break
        print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
    
    print(f"\nğŸ¯ æ‚¨é€‰æ‹©äº†: {selected_model['name']}")
    print(f"ğŸ“Š é¢„è®¡ä¸‹è½½å¤§å°: {selected_model['size']}")
    print(f"ğŸ’¾ å†…å­˜è¦æ±‚: {selected_model['ram_requirement']}")
    
    confirm = input("\nç¡®è®¤ä¸‹è½½? (y/N): ").strip().lower()
    if confirm != 'y':
        print("âŒ ä¸‹è½½å·²å–æ¶ˆ")
        return 0
    
    # ä¸‹è½½æ¨¡å‹
    model, tokenizer, model_path = download_model(selected_model['name'])
    
    if model is None:
        print("âŒ æ¨¡å‹ä¸‹è½½å¤±è´¥")
        return 1
    
    # è½¬æ¢å’Œå®‰è£…
    vocab_file, info_file = convert_to_tflite(
        model, tokenizer, selected_model['name']
    )
    
    if vocab_file is None:
        print("âŒ æ¨¡å‹è½¬æ¢å¤±è´¥")
        return 1
    
    # åˆ›å»ºä¼˜åŒ–è®¾ç½®
    create_optimized_setup()
    
    print("\n" + "=" * 50)
    print("ğŸ‰ Gemma 3N æ¨¡å‹è®¾ç½®å®Œæˆ!")
    print("\nğŸ“ å·²åˆ›å»ºçš„æ–‡ä»¶:")
    print(f"   â€¢ {vocab_file}")
    print(f"   â€¢ {info_file}")
    
    print("\nğŸ“± ä¸‹ä¸€æ­¥:")
    print("   1. ç¼–è¯‘ Android é¡¹ç›®: ./gradlew assembleDebug")
    print("   2. æ ¹æ®éœ€è¦é›†æˆçœŸå®æ¨¡å‹æ¨ç†é€»è¾‘")
    print("   3. æµ‹è¯•åº”ç”¨åŠŸèƒ½")
    
    print("\nğŸ’¡ æç¤º:")
    print("   â€¢ æ¨¡å‹å·²ä¸‹è½½ä½†éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–æ‰èƒ½åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šè¿è¡Œ")
    print("   â€¢ å»ºè®®ä½¿ç”¨ MediaPipe æˆ–å…¶ä»–ç§»åŠ¨ä¼˜åŒ–æ¡†æ¶")
    print("   â€¢ å¯ä»¥å…ˆä½¿ç”¨å½“å‰è®¾ç½®æµ‹è¯•åº”ç”¨çš„å…¶ä»–åŠŸèƒ½")
    
    return 0

if __name__ == "__main__":
    exit(main())
